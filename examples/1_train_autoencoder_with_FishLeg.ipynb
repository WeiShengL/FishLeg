{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Autoencoder with FishLeg\n",
    "\n",
    "FishLeg is a second-order optimizer for training neural networks. This means FishLeg uses second-order information - or the curvature of weight-space - to make more optimal adjustments to weights during neural network training. This allows FishLeg to reach lower losses with fewer epochs. This notebook demonstrates using FishLeg to train a basic autoencoder, working with the MNIST dataset of handwritten digits, and compares the training performance with Adam, a popular first-order neural network optimizer.\n",
    "\n",
    "FishLeg was first introduced by [Garcia et. al.](https://openreview.net/forum?id=c9lAOPvQHS) at ICLR 2023. Please see the official [GitHub page](https://github.com/mtkresearch/FishLeg) for more examples, explanations and for citing FishLeg.\n",
    "\n",
    "### Step 0: Install the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Imports and notebook preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm # tqdm ('te quiero demasiado') creates progress bars for loops\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from data_utils import read_data_sets # Accessing examples/data_utils.py\n",
    "import copy\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "seed = 13\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False \n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: GPU Acceleration\n",
    "Depending on your system, PyTorch can use one of several hardware accelerations for training. For efficiency, we will select the most powerful device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): # i.e. for NVIDIA GPUs\n",
    "    device_type = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): # i.e. for Apple Silicon GPUs\n",
    "    device_type = \"mps\"\n",
    "else:\n",
    "    device_type = \"cpu\"\n",
    "    \n",
    "device = torch.device(device_type) # Select best available device\n",
    "print(f'Running on device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Import the MNIST Dataset\n",
    "\n",
    "MNIST is a famous dataset which contains examples of handwritten digits. The dataset consists of 60,0000 training images and 10,000 test images. More can information can be found in the MNIST [homepage](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "The dataset is split into train and test datasets. We then prepare test, train and auxiliary dataloaders, which are iterator objects that allow for passing shuffled and batched data into PyTorch. The auxiliary dataloader loads the same training data, however shuffled differently, and it is used in a meta-learning approach during the FishLeg optimisation. This additional dataloader is an important requirement for training with FishLeg.\n",
    "\n",
    "No data augmentation is performed in this notebook, but we invite users to experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose to access MNIST using the data_utils.py function read_data_sets. \n",
    "# This prepares the dataset for use in this example notebook in a concise manner.\n",
    "dataset = read_data_sets(\"MNIST\", \"../data/\", if_autoencoder=True)\n",
    "train_dataset = dataset.train # Accessing the training dataset\n",
    "test_dataset = dataset.test # Accessing the test dataset\n",
    "\n",
    "batch_size = 100 # Setting the batch size for training and auxiliary dataloaders\n",
    "\n",
    "# Creating the dataloader for the training data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# Creating the dataloader for the auxiliary data\n",
    "aux_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)),\n",
    ")\n",
    "\n",
    "# Creating the dataloader for the test data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1000, shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the data inside the MNIST dataset by plotting a random selection of the images that will be used to test the trained model. \n",
    "\n",
    "Run this cell multiple times to observe the variance of the hand-written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5,5, figsize=(8,8))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    rand = np.random.randint(0,9999)\n",
    "    ax.imshow(np.array(test_dataset[rand])[0].reshape(28,28), cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create autoencoder model\n",
    "\n",
    "Here we create our basic autoencoder, `model`. We then initialise two copies of the model, one to be trained with the FishLeg optimizer and the other to be trained with <span style='color:DeepSkyBlue'>Adam</span>, for comparison.\n",
    "\n",
    "The architecture of the autoencoder model is as follows:\n",
    "| Code | Explanation |\n",
    "| :- | :- |\n",
    "| `nn.Linear(784, 1000, dtype=torch.float32)`             | Input layer: Takes in a flattened 28x28 pixel (784 values) image as input and outputs a 1000-dimensional vector |\n",
    "| `nn.ReLU()`                                             | Activation function: Applies the ReLU (Rectified Linear Unit) function the output of the previous layer to introduce non-linearity |\n",
    "|`nn.Linear(1000, 500, dtype=torch.float32)`            | Hidden layer: Takes the 1000-dimensional vector and outputs a 500-dimensional vector |\n",
    "| `nn.ReLU()` | |\n",
    "| `nn.Linear(500, 250, dtype=torch.float32)`          |    Hidden layer: Takes the 500-dimensional vector and reduces the output down to a 250-dimensional vector |\n",
    "| `nn.ReLU()` | |\n",
    "| `nn.Linear(250, 30, dtype=torch.float32)`            |   Waist layer: Takes the 250-dimensional vector and outputs a 30-dimensional vector. This is the smallest compressed representation of the data, every possible feature should be able to be described with no less than 30 degrees of freedom. |\n",
    "| `nn.Linear(30, 250, dtype=torch.float32)`     |          Start of the decoder part of the network: Takes the 30-dimensional vector and upscales to a 250-dimensional vector |\n",
    "| `nn.ReLU()` | |\n",
    "| `nn.Linear(250, 500, dtype=torch.float32)`       |       Hidden layer: Takes the 250-dimensional vector and upscales again to a 500-dimensional vector |\n",
    "| `nn.ReLU()` | |\n",
    "| `nn.Linear(500, 1000, dtype=torch.float32)`     |        Hidden layer: Takes the 500-dimensional vector and outputs a 1000-dimensional vector |\n",
    "| `nn.ReLU()` | |\n",
    "| `nn.Linear(1000, 784, dtype=torch.float32)`      |       Output layer: Takes the 1000-dimensional vector and outputs the 784-dimensional vector, which is the same size as the input. This is the reconstructed image. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 1000, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 500, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 250, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(250, 30, dtype=torch.float32),\n",
    "    nn.Linear(30, 250, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(250, 500, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 1000, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 784, dtype=torch.float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Initialising FishLeg optimizer\n",
    "\n",
    "Here we prepare FishLeg for optimizing our autoencoder. To do so, we first need to modify our autoencoder model by calling `initialise_FishModel`. This replaces the layer objects in the above model with modified versions that contain additional parameters that are necessary for calculations performed by FishLeg. As with many optimizers, FishLeg has a selection of hyperparameters that can be tuned according to the problem at hand. FishLeg shares similar hyperparameters with other optimizers, however it has additional hyperparameters for the online, meta-learning that FishLeg performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim.FishLeg import FishLeg, FISH_LIKELIHOODS, initialise_FishModel\n",
    "\n",
    "## Create FishLeg autoencoder model ##\n",
    "model_FishLeg = copy.deepcopy(model).to(device) # Create a deep copy of our model for training with FishLeg\n",
    "\n",
    "scale_factor = 1\n",
    "damping = 0.1\n",
    "model_FishLeg = initialise_FishModel(\n",
    "    model_FishLeg, module_names=\"__ALL__\", fish_scale=scale_factor / damping\n",
    ")\n",
    "\n",
    "model_FishLeg = model_FishLeg.to(device) # Select device to train our model on\n",
    "\n",
    "\n",
    "## Setting FishLeg optimizer parameters ##\n",
    "lr = 0.005\n",
    "beta = 0.9\n",
    "weight_decay = 1e-5\n",
    "aux_lr = 1e-4\n",
    "aux_eps = 1e-8\n",
    "update_aux_every = 10\n",
    "initialization = \"normal\"\n",
    "normalization = True\n",
    "likelihood = FISH_LIKELIHOODS[\"bernoulli\"](device=device) # Our selected function to evaluate the negative log-likelihood\n",
    "writer = SummaryWriter(\n",
    "    log_dir=f\"runs/MNIST_fishleg/lr={lr}_auxlr={aux_lr}/{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    ")\n",
    "\n",
    "## Initialising FishLeg ##\n",
    "opt = FishLeg(\n",
    "    model_FishLeg,\n",
    "    aux_loader,\n",
    "    likelihood,\n",
    "    lr=lr,\n",
    "    beta=beta,\n",
    "    weight_decay=weight_decay,\n",
    "    aux_lr=aux_lr,\n",
    "    aux_betas=(0.9, 0.999),\n",
    "    aux_eps=aux_eps,\n",
    "    damping=damping,\n",
    "    update_aux_every=update_aux_every,\n",
    "    writer=writer,\n",
    "    method=\"antithetic\",\n",
    "    method_kwargs={\"eps\": 1e-4},\n",
    "    precondition_aux=True,\n",
    "    aux_log=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Training with FishLeg:\n",
    "\n",
    "Here we train our model using FishLeg using an arbitrary training loop. This training loop is identical to training with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "st = time.time()\n",
    "eval_time = 0\n",
    "\n",
    "train_losses_FishLeg = [] # Saving our losses for comparison...\n",
    "test_losses_FishLeg = []\n",
    "auxiliary_losses = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        running_loss = 0\n",
    "        for n, (batch_data, batch_labels) in enumerate(tepoch, start=1):\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            output = model_FishLeg(batch_data)\n",
    "\n",
    "            loss = likelihood(output, batch_labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            auxiliary_losses.append(opt.aux_loss)\n",
    "\n",
    "            et = time.time()\n",
    "            if n % 50 == 0:\n",
    "                model_FishLeg.eval()\n",
    "\n",
    "                running_test_loss = 0\n",
    "\n",
    "                for m, (test_batch_data, test_batch_labels) in enumerate(test_loader):\n",
    "                    test_batch_data, test_batch_labels = test_batch_data.to(\n",
    "                        device\n",
    "                    ), test_batch_labels.to(device)\n",
    "\n",
    "                    test_output = model_FishLeg(test_batch_data)\n",
    "\n",
    "                    test_loss = likelihood(test_output, test_batch_labels)\n",
    "\n",
    "                    running_test_loss += test_loss.item()\n",
    "\n",
    "                running_test_loss /= m\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item(), test_loss=running_test_loss)\n",
    "                model_FishLeg.train()\n",
    "                eval_time += time.time() - et\n",
    "\n",
    "        epoch_time = time.time() - st - eval_time\n",
    "\n",
    "        tepoch.set_postfix(\n",
    "            loss=running_loss / n, test_loss=running_test_loss, epoch_time=epoch_time\n",
    "        )\n",
    "\n",
    "        train_losses_FishLeg.append(running_loss / n)\n",
    "        test_losses_FishLeg.append(running_test_loss)\n",
    "\n",
    "        # Write out the losses per epoch\n",
    "        writer.add_scalar(\"Loss/train\", running_loss / n, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", running_test_loss, epoch)\n",
    "\n",
    "        # Write out the losses per wall clock time\n",
    "        writer.add_scalar(\"Loss/train/time\", running_loss / n, epoch_time)\n",
    "        writer.add_scalar(\"Loss/test/time\", running_test_loss, epoch_time)\n",
    "\n",
    "print(f\"training time: {(time.time() - st) / 60 :.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Initialising Adam optimizer:\n",
    "\n",
    "Here we prepare Adam for optimising our autoencoder. This step, similarly to the FishLeg initialisation, requires defining various hyperparameters for the Adam optimiser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create ADAM autoencoder model ##\n",
    "model_ADAM = copy.deepcopy(model).to(device) # Create a deep copy of our model for training with ADAM\n",
    "\n",
    "## Setting ADAM optimizer parameters ##\n",
    "lr = 0.005\n",
    "weight_decay = 1e-5\n",
    "likelihood = FISH_LIKELIHOODS[\"bernoulli\"](device=device)\n",
    "\n",
    "## Initialising ADAM ##\n",
    "opt = optim.Adam(\n",
    "    model_ADAM.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(\n",
    "    log_dir=f\"runs/MNIST_adam/lr={lr}_lambda={weight_decay}/{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Training with Adam:\n",
    "\n",
    "This training loop is identical to training with FishLeg, except Adam does not utilise an auxiliary loss, so we do not track that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "st = time.time()\n",
    "eval_time = 0\n",
    "\n",
    "train_losses_ADAM = [] # Saving our losses for comparison...\n",
    "test_losses_ADAM = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        running_loss = 0\n",
    "        for n, (batch_data, batch_labels) in enumerate(tepoch, start=1):\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            output = model_ADAM(batch_data)\n",
    "\n",
    "            loss = likelihood(output, batch_labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            et = time.time()\n",
    "            if n % 50 == 0:\n",
    "                model_ADAM.eval()\n",
    "\n",
    "                running_test_loss = 0\n",
    "\n",
    "                for m, (test_batch_data, test_batch_labels) in enumerate(test_loader):\n",
    "                    test_batch_data, test_batch_labels = test_batch_data.to(\n",
    "                        device\n",
    "                    ), test_batch_labels.to(device)\n",
    "\n",
    "                    test_output = model_ADAM(test_batch_data)\n",
    "\n",
    "                    test_loss = likelihood(test_output, test_batch_labels)\n",
    "\n",
    "                    running_test_loss += test_loss.item()\n",
    "\n",
    "                running_test_loss /= m\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item(), test_loss=running_test_loss)\n",
    "                model_ADAM.train()\n",
    "                eval_time += time.time() - et\n",
    "\n",
    "        epoch_time = time.time() - st - eval_time\n",
    "\n",
    "        tepoch.set_postfix(\n",
    "            loss=running_loss / n, test_loss=running_test_loss, epoch_time=epoch_time\n",
    "        )\n",
    "\n",
    "        train_losses_ADAM.append(running_loss / n)\n",
    "        test_losses_ADAM.append(running_test_loss)\n",
    "        \n",
    "        # Write out the losses per epoch\n",
    "        writer.add_scalar(\"Loss/train\", running_loss / n, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", running_test_loss, epoch)\n",
    "\n",
    "        # Write out the losses per wall clock time\n",
    "        writer.add_scalar(\"Loss/train/time\", running_loss / n, epoch_time)\n",
    "        writer.add_scalar(\"Loss/test/time\", running_test_loss, epoch_time)\n",
    "\n",
    "print(f\"training time: {(time.time() - st) / 60 :.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: FishLeg versus Adam:\n",
    "\n",
    "Now we will look at how the loss improves over the training period for FishLeg and Adam. In the plot below, we can see that after training for 100 epochs, FishLeg converges to a lower training loss than Adam. In this case, the training time of FishLeg was a few minutes longer, but it took just 5 epochs for FishLeg to reach a lower loss than Adam converges to in 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5), dpi = 250)\n",
    "\n",
    "plt.plot(train_losses_ADAM, 'tab:blue',label=\"ADAM\")\n",
    "plt.plot(train_losses_FishLeg, 'tab:orange',label=\"FishLeg\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\") \n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot below, we see a similar trend for the test loss. This indicates that the FishLeg model is generalising well to unseen test data. Furthermore, the test loss is more stable for the FishLeg model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5), dpi = 250)\n",
    "\n",
    "plt.plot(test_losses_ADAM, 'tab:blue',label=\"ADAM\", linestyle=\"dashed\")\n",
    "plt.plot(test_losses_FishLeg, 'tab:orange',label=\"FishLeg\", linestyle=\"dashed\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Test Loss\") \n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is FishLeg better approximating the Fisher matrix over the training period? We can see below that as our auxiliary loss improves over time, showing that the approximation for the Fisher is improving as our training iterates. Plotting the test loss alongside the auxiliary loss, we can see that the test loss improves in accordance with the improvements in auxiliary loss. However, the auxiliary loss is far more noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_losses = [i.to(\"cpu\") for i in auxiliary_losses if i]\n",
    "x1 = np.linspace(1,100,59990)\n",
    "x2 = np.linspace(1,100,100)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5), dpi = 250)\n",
    "\n",
    "color = 'tab:green'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Auxiliary Loss', color=color)\n",
    "ax1.plot(x1, auxiliary_losses, color=color, linewidth=0.5)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylim((-600,600))\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('Training Loss', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(x2, train_losses_FishLeg, color=color, linewidth=3)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean for the reconstructed images? Lets compare the reconstructions produced by FishLeg against those produced by Adam. Below we have plotted three random images from the test set along with their FishLeg and Adam reconstructions. For each reconstruction, the mean squared error (MSE) has been calculated (lower is better, with the best being zero), along with a second plot below that illustrates the sources of errors. The closer these second plots are to the plain blue test plot (i.e. zero squared error everywhere), the closer the reconstructed image is to the true image.\n",
    "\n",
    "Run this cell multiple times to observe results for different random images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss() # PyTorch implementation of mean squared error\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    i=np.random.randint(0,9999)\n",
    "    cmap1 = 'Greys_r'\n",
    "    cmap2 = 'cividis'\n",
    "\n",
    "    test_image = test_dataset[i][0]\n",
    "    test_image_numpy = np.array(test_image)\n",
    "    fishleg_reconstruction = torch.sigmoid(model_FishLeg(test_image.to(device))).cpu().detach()\n",
    "    adam_reconstruction = torch.sigmoid(model_ADAM(test_image.to(device))).cpu().detach()\n",
    "\n",
    "    fig, axs = plt.subplots(2,3, figsize=(8.6,4.5))\n",
    "    axs=axs.flatten()\n",
    "\n",
    "    axs[0].imshow(test_image_numpy.reshape(28,28), cmap=cmap1)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(f\"Test Image\\n\")\n",
    "\n",
    "    axs[1].imshow(np.array(fishleg_reconstruction).reshape(28,28), cmap=cmap1)\n",
    "    axs[1].axis('off')\n",
    "    mse_fishleg = mse(fishleg_reconstruction,test_image)\n",
    "    axs[1].set_title(\"FishLeg\\n Reconstruction\\n (MSE: {:.2e})\".format(mse_fishleg))\n",
    "\n",
    "    axs[2].imshow(np.array(adam_reconstruction).reshape(28,28), cmap=cmap1)\n",
    "    axs[2].axis('off')\n",
    "    mse_adam = mse(adam_reconstruction,test_image)\n",
    "    axs[2].set_title(\"ADAM\\n Reconstruction\\n (MSE: {:.2e})\".format(mse_adam))\n",
    "\n",
    "    err = lambda x: (x-test_image_numpy)**2 # Calculate squared error\n",
    "\n",
    "    axs[3].imshow(err(test_image_numpy).reshape(28,28), cmap=cmap2, vmin=0, vmax=1)\n",
    "    axs[3].axis('off')\n",
    "\n",
    "    axs[4].imshow(err(fishleg_reconstruction).reshape(28,28), cmap=cmap2, vmin=0, vmax=1)\n",
    "    axs[4].axis('off')\n",
    "\n",
    "    im = axs[5].imshow(err(adam_reconstruction).reshape(28,28), cmap=cmap2, vmin=0, vmax=1)\n",
    "    axs[5].axis('off')\n",
    "    \n",
    "    fig.colorbar(im, ax=axs[5])\n",
    "\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FishLeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
