{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Autoencoder with FishLeg versus ADAM\n",
    "\n",
    "FishLeg is a second order optimizer for training neural networks. This notebook demonstrates using FishLeg to train an autoencoder, working with the MNIST dataset of handwritten digits, and compares the training performance with ADAM, a common neural network optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm # tqdm ('te quiero demasiado') creates progress bars for loops\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from data_utils import read_data_sets\n",
    "import copy\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32) # Set the precision of torch tensors to 32 bit floats\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Select the GPU to use, in this case there is only 1, so the first one\n",
    "\n",
    "seed = 13\n",
    "torch.manual_seed(seed) # Specify the seed for generating random numbers, for reproducibility of results:\n",
    "torch.backends.cudnn.benchmark = False # ??\n",
    "torch.backends.cudnn.deterministic = True # Allowing reproducibility of results if we are using CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Acceleration\n",
    "Depending on your system, PyTorch can use one of several hardware accelerations for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): # i.e. for NVIDIA GPUs\n",
    "    device_type = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): # i.e. for Apple Silicon GPUs\n",
    "    device_type = \"mps\"\n",
    "else:\n",
    "    device_type = \"cpu\"\n",
    "    \n",
    "device = torch.device(device_type)\n",
    "print(f'Running on device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the MNIST Dataset\n",
    "\n",
    "MNIST is a famous dataset which contains examples of handwritten digits. More can be found [here](https://www.tensorflow.org/datasets/catalog/mnist).\n",
    "\n",
    "The dataset is prepared for training by dividing it into testing and training groups, and also initialising dataloaders, which are iterator objects with automatic batching support that allow for passing the data into PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin loading data for MNIST\n",
      "Data read from ../data/data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Data read from ../data/data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Succesfully loaded MNIST dataset.\n"
     ]
    }
   ],
   "source": [
    "dataset = read_data_sets(\"MNIST\", \"../data/\", if_autoencoder=True)\n",
    "\n",
    "# the dataset is already split into test and train groups\n",
    "train_dataset = dataset.train\n",
    "test_dataset = dataset.test\n",
    "\n",
    "# using batches of 100\n",
    "batch_size = 100\n",
    "\n",
    "#this is the main loader for the loop, it splits the data into batches of 100 randomly shuffled data entries.\n",
    "# MNIST has 60,000 train data entries so this is 600 batches\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "# the aux dataloader is used for the fischer to learn on the same train data. This is shuffled differently\n",
    "# to the train loader, so that the batches are different.\n",
    "aux_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, shuffle=True, batch_size=batch_size, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)),\n",
    ")\n",
    "# the test loader does not need to be shuffled as the order of testing doesnt impact the validation. There\n",
    "# are 10,000 test data entries so with a batch of 1000 this is 10 test batches\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1000, shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create autoencoder model\n",
    "\n",
    "Here we create our autoencoder, `model`. We then initialise two copies of the model, one to be trained with the FishLeg optimizer and the other to be trained with ADAM, for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the simple autoencoder model is as follows:\n",
    "| Code | Explanation |\n",
    "| :- | :- |\n",
    "| `nn.Linear(784, 1000, dtype=torch.float32)`             | Input layer: Takes in a flattened 28x28 pixel (784 values) image as input and outputs a 1000-dimensional vector |\n",
    "| `nn.ReLU()`                                             | Activation function: Applies the ReLU (Rectified Linear Unit) function to introduce non-linearity |\n",
    "|`nn.Linear(1000, 500, dtype=torch.float32)`            | Hidden layer: Takes the 1000-dimensional vector and outputs a 500-dimensional vector |\n",
    "| `nn.ReLU()` | |\n",
    "| `nn.Linear(500, 250, dtype=torch.float32)`          |    Hidden layer: Takes the 500-dimensional vector and reduces the output down to a 250-dimensional vector |\n",
    "| `nn.ReLU()` | |\n",
    "| `nn.Linear(250, 30, dtype=torch.float32)`            |   Waist layer: Takes the 250-dimensional vector and outputs a 30-dimensional vector. This is the smallest compressed representation of the data, every possible feature should be able to be described with no less than 30 degrees of freedom. |\n",
    "| `nn.Linear(30, 250, dtype=torch.float32)`     |          Start of the decoder part of the network: Takes the 30-dimensional vector and upscales to a 250-dimensional vector |\n",
    "| `nn.ReLU()` | |\n",
    "| `nn.Linear(250, 500, dtype=torch.float32)`       |       Hidden layer: Takes the 250-dimensional vector and upscales again to a 500-dimensional vector |\n",
    "| `nn.ReLU()` | |\n",
    "| `nn.Linear(500, 1000, dtype=torch.float32)`     |        Hidden layer: Takes the 500-dimensional vector and outputs a 1000-dimensional vector |\n",
    "| `nn.ReLU()` | |\n",
    "| `nn.Linear(1000, 784, dtype=torch.float32)`      |       Output layer: Takes the 1000-dimensional vector and outputs the 784-dimensional vector, which is the same size as the input. This is the reconstructed image. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 1000, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 500, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 250, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(250, 30, dtype=torch.float32),\n",
    "    nn.Linear(30, 250, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(250, 500, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 1000, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 784, dtype=torch.float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising FishLeg optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim.FishLeg import FishLeg, FISH_LIKELIHOODS, initialise_FishModel\n",
    "# Define the FishLeg model\n",
    "# For compatibility with FishLeg, our model layers need to be initialised with additional parameters. This is completed by initialise_FishModel():\n",
    "scale_factor = 1\n",
    "damping = 0.1\n",
    "# Specify the hardware device that will be used to train the model\n",
    "model_FishLeg = copy.deepcopy(model).to(device)\n",
    "model_FishLeg = initialise_FishModel(\n",
    "    model_FishLeg, module_names=\"__ALL__\", fish_scale=scale_factor / damping\n",
    ")\n",
    "model_FishLeg = model_FishLeg.to(device)\n",
    "# Setting FishLeg optimizer parameters:\n",
    "eta_adam = 1e-4\n",
    "lr = 0.005\n",
    "beta = 0.9\n",
    "weight_decay = 1e-5\n",
    "aux_lr = 1e-4\n",
    "aux_eps = 1e-8\n",
    "update_aux_every = 10\n",
    "initialization = \"normal\"\n",
    "normalization = True\n",
    "likelihood = FISH_LIKELIHOODS[\"bernoulli\"](device=device)\n",
    "writer = SummaryWriter(\n",
    "    log_dir=f\"runs/MNIST_fishleg/lr={lr}_auxlr={aux_lr}/{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    ")\n",
    "# Initialising FishLeg:\n",
    "opt = FishLeg(\n",
    "    model_FishLeg,\n",
    "    aux_loader,\n",
    "    likelihood,\n",
    "    lr=lr,\n",
    "    beta=beta,\n",
    "    weight_decay=weight_decay,\n",
    "    aux_lr=aux_lr,\n",
    "    aux_betas=(0.9, 0.999),\n",
    "    aux_eps=aux_eps,\n",
    "    damping=damping,\n",
    "    update_aux_every=update_aux_every,\n",
    "    writer=writer,\n",
    "    method=\"antithetic\",\n",
    "    method_kwargs={\"eps\": 1e-4},\n",
    "    precondition_aux=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with FishLeg:\n",
    "\n",
    "This training loop is identical to training with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 600/600 [00:18<00:00, 32.31batch/s, loss=118, test_loss=128]\n",
      "Epoch 2: 100%|██████████| 600/600 [00:17<00:00, 33.61batch/s, loss=88.9, test_loss=96.6]\n",
      "Epoch 3:  94%|█████████▍| 566/600 [00:16<00:01, 33.73batch/s, loss=77.3, test_loss=85.3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, (batch_data, batch_labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tepoch, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     13\u001b[0m     tepoch\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     batch_data, batch_labels \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, batch_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m     output \u001b[38;5;241m=\u001b[39m model_FishLeg(batch_data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "st = time.time()\n",
    "eval_time = 0\n",
    "\n",
    "train_losses_FishLeg = []\n",
    "test_losses_FishLeg = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        running_loss = 0\n",
    "        for n, (batch_data, batch_labels) in enumerate(tepoch, start=1):\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            output = model_FishLeg(batch_data)\n",
    "\n",
    "            loss = likelihood(output, batch_labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            et = time.time()\n",
    "            if n % 50 == 0:\n",
    "                model_FishLeg.eval()\n",
    "\n",
    "                running_test_loss = 0\n",
    "\n",
    "                for m, (test_batch_data, test_batch_labels) in enumerate(test_loader):\n",
    "                    test_batch_data, test_batch_labels = test_batch_data.to(\n",
    "                        device\n",
    "                    ), test_batch_labels.to(device)\n",
    "\n",
    "                    test_output = model_FishLeg(test_batch_data)\n",
    "\n",
    "                    test_loss = likelihood(test_output, test_batch_labels)\n",
    "\n",
    "                    running_test_loss += test_loss.item()\n",
    "\n",
    "                running_test_loss /= m\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item(), test_loss=running_test_loss)\n",
    "                model_FishLeg.train()\n",
    "                eval_time += time.time() - et\n",
    "\n",
    "        epoch_time = time.time() - st - eval_time\n",
    "\n",
    "        tepoch.set_postfix(\n",
    "            loss=running_loss / n, test_loss=running_test_loss, epoch_time=epoch_time\n",
    "        )\n",
    "\n",
    "        train_losses_FishLeg.append(running_loss / n)\n",
    "        test_losses_FishLeg.append(running_test_loss)\n",
    "\n",
    "        # Write out the losses per epoch\n",
    "        writer.add_scalar(\"Loss/train\", running_loss / n, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", running_test_loss, epoch)\n",
    "\n",
    "        # Write out the losses per wall clock time\n",
    "        writer.add_scalar(\"Loss/train/time\", running_loss / n, epoch_time)\n",
    "        writer.add_scalar(\"Loss/test/time\", running_test_loss, epoch_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising ADAM optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the control ADAM model\n",
    "model_ADAM = copy.deepcopy(model).to(device)\n",
    "\n",
    "lr = 0.005\n",
    "# betas = (0.7, 0.9)\n",
    "weight_decay = 1e-5\n",
    "# eps = 1e-8\n",
    "likelihood = FISH_LIKELIHOODS[\"bernoulli\"](device=device)\n",
    "\n",
    "opt = optim.Adam(\n",
    "    model_ADAM.parameters(),\n",
    "    lr=lr,\n",
    "    # betas=betas,\n",
    "    weight_decay=weight_decay,\n",
    "    # eps=eps,\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(\n",
    "    log_dir=f\"runs/MNIST_adam/lr={lr}_lambda={weight_decay}/{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with ADAM:\n",
    "\n",
    "This training loop is identical to training with FishLeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "st = time.time()\n",
    "eval_time = 0\n",
    "\n",
    "train_losses_ADAM = []\n",
    "test_losses_ADAM = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        running_loss = 0\n",
    "        for n, (batch_data, batch_labels) in enumerate(tepoch, start=1):\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            output = model_ADAM(batch_data)\n",
    "\n",
    "            loss = likelihood(output, batch_labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            et = time.time()\n",
    "            if n % 50 == 0:\n",
    "                model_ADAM.eval()\n",
    "\n",
    "                running_test_loss = 0\n",
    "\n",
    "                for m, (test_batch_data, test_batch_labels) in enumerate(test_loader):\n",
    "                    test_batch_data, test_batch_labels = test_batch_data.to(\n",
    "                        device\n",
    "                    ), test_batch_labels.to(device)\n",
    "\n",
    "                    test_output = model_ADAM(test_batch_data)\n",
    "\n",
    "                    test_loss = likelihood(test_output, test_batch_labels)\n",
    "\n",
    "                    running_test_loss += test_loss.item()\n",
    "\n",
    "                running_test_loss /= m\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item(), test_loss=running_test_loss)\n",
    "                model_ADAM.train()\n",
    "                eval_time += time.time() - et\n",
    "\n",
    "        epoch_time = time.time() - st - eval_time\n",
    "\n",
    "        tepoch.set_postfix(\n",
    "            loss=running_loss / n, test_loss=running_test_loss, epoch_time=epoch_time\n",
    "        )\n",
    "\n",
    "        train_losses_ADAM.append(running_loss / n)\n",
    "        test_losses_ADAM.append(running_test_loss)\n",
    "        \n",
    "        # Write out the losses per epoch\n",
    "        writer.add_scalar(\"Loss/train\", running_loss / n, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", running_test_loss, epoch)\n",
    "\n",
    "        # Write out the losses per wall clock time\n",
    "        writer.add_scalar(\"Loss/train/time\", running_loss / n, epoch_time)\n",
    "        writer.add_scalar(\"Loss/test/time\", running_test_loss, epoch_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FishLeg versus ADAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(test_losses_ADAM, 'k--',label=\"ADAM\")\n",
    "plt.plot(test_losses_FishLeg, 'g',label=\"FishLeg\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\") \n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.array(test_dataset.__getitem__(0)).reshape(28,28), cmap='gray')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset\n",
    "\n",
    "for i in range (len(test_dataset)):\n",
    "    plt.imshow(test_dataset[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(test_dataset.__getitem__(0)).reshape(28,28), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FishLeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
