{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Autoencoder with FishLeg versus ADAM\n",
    "\n",
    "FishLeg is a second order optimizer for training neural networks. This notebook demonstrates using FishLeg to train an autoencoder, working with the MNIST dataset of handwritten digits, and compares the training performance with ADAM, a common neural network optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch            # import pytorch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm   # tqdm ('te quiero demasiado') creates progress bars for loops\n",
    "import time\n",
    "import os               # for file manipulation and checking the file structure is correct\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter   # tensorboard creates a local web server to analyse training runs (it stores temp files in `examples/runs`)\n",
    "from data_utils import read_data_sets   # this allows for reading in the MNIST data from a web source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the precision of torch tensors to 32 bit floats:\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "#Specify the seed for generating random numbers, for reproducibility of results:\n",
    "seed = 13\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#Select GPU if it is available:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Acceleration\n",
    "Depending on your system, PyTorch can use one of several hardware accelerations for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): # i.e. for NVIDIA GPUs\n",
    "    device_type = \"cuda\" \n",
    "elif torch.backends.mps.is_available(): # i.e. for Apple Silicon GPUs\n",
    "    device_type = \"mps\"\n",
    "else:\n",
    "    device_type = \"cpu\"\n",
    "\n",
    "device = torch.device(device_type)\n",
    "print(f'Running on device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the MNIST Dataset\n",
    "\n",
    "MNIST is a famous dataset which contains examples of handwritten digits. More can be found [here](https://www.tensorflow.org/datasets/catalog/mnist).\n",
    "\n",
    "The dataset is prepared for training by dividing it into testing and training groups, and also initialising dataloaders, which are iterator objects with automatic batching support that allow for passing the data into PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_data_sets(\"MNIST\", \"../data/\", if_autoencoder=True)\n",
    "\n",
    "# the dataset is already split into test and train groups\n",
    "train_dataset = dataset.train\n",
    "test_dataset = dataset.test\n",
    "\n",
    "# using batches of 100 \n",
    "batch_size = 100\n",
    "\n",
    "#this is the main loader for the loop, it splits the data into batches of 100 randomly shuffled data entries.\n",
    "# MNIST has 60,000 train data entries so this is 600 batches\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# the aux dataloader is used for the fischer to learn on the same train data. This is shuffled differently\n",
    "# to the train loader, so that the batches are different. \n",
    "aux_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, shuffle=True, batch_size=batch_size\n",
    ")\n",
    "\n",
    "# the test loader does not need to be shuffled as the order of testing doesnt impact the validation. There \n",
    "# are 10,000 test data entries so with a batch of 1000 this is 10 test batches\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1000, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create autoencoder model\n",
    "\n",
    "Here we create our autoencoder, `model`. We then initialise two copies of the model, one to be trained with the FishLeg optimizer and the other to be trained with ADAM, for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Stretch/Untouched | ProbDistribution |\n",
    "| --- | --- |\n",
    "| Stretched | Gaussian |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the simple autoencoder model is as follows:\n",
    "\n",
    "| `nn.Linear(784, 1000, dtype=torch.float32)`             | Input layer: Takes in a flattened 28x28 pixel (784 values) image as input and outputs a 1000-dimensional vector |\n",
    "\n",
    "| `nn.ReLU()`                                             | Activation function: Applies the ReLU (Rectified Linear Unit) function to introduce non-linearity |\n",
    "\n",
    "`nn.Linear(1000, 500, dtype=torch.float32)`             Hidden layer: Takes the 1000-dimensional vector and outputs a 500-dimensional vector\n",
    "\n",
    "`nn.ReLU()`\n",
    "\n",
    "`nn.Linear(500, 250, dtype=torch.float32)`              Hidden layer: Takes the 500-dimensional vector and reduces the output down to a 250-dimensional vector\n",
    "\n",
    "`nn.ReLU()` \n",
    "\n",
    "`nn.Linear(250, 30, dtype=torch.float32)`               Waist layer: Takes the 250-dimensional vector and outputs a 30-dimensional vector. This is the smallest compressed representation of the data, every possible feature should be able to be described with no less than 30 degrees of freedom.\n",
    "\n",
    "`nn.Linear(30, 250, dtype=torch.float32)`               Start of the decoder part of the network: Takes the 30-dimensional vector and upscales to a 250-dimensional vector\n",
    "\n",
    "`nn.ReLU()` \n",
    "\n",
    "`nn.Linear(250, 500, dtype=torch.float32)`              Hidden layer: Takes the 250-dimensional vector and upscales again to a 500-dimensional vector\n",
    "\n",
    "`nn.ReLU()` \n",
    "\n",
    "`nn.Linear(500, 1000, dtype=torch.float32)`             Hidden layer: Takes the 500-dimensional vector and outputs a 1000-dimensional vector\n",
    "\n",
    "`nn.ReLU()` \n",
    "\n",
    "`nn.Linear(1000, 784, dtype=torch.float32)`             Output layer: Takes the 1000-dimensional vector and outputs the 784-dimensional vector, which is the same size as the input. This is the reconstructed image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 1000, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 500, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 250, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(250, 30, dtype=torch.float32),\n",
    "    nn.Linear(30, 250, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(250, 500, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 1000, dtype=torch.float32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 784, dtype=torch.float32),\n",
    ")\n",
    "\n",
    "from optim.FishLeg import FishLeg, FISH_LIKELIHOODS, initialise_FishModel\n",
    "\n",
    "# For compatibility with FishLeg, our model layers need to be initialised with additional parameters. This is completed by initialise_FishModel():\n",
    "scale_factor = 1\n",
    "damping = 0.1\n",
    "\n",
    "model_FishLeg = initialise_FishModel(\n",
    "    model, module_names=\"__ALL__\", fish_scale=scale_factor / damping\n",
    ")\n",
    "\n",
    "# Specify the hardware device that will be used to train the model\n",
    "model_FishLeg = model_FishLeg.to(device)\n",
    "\n",
    "model_ADAM = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising FishLeg optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting FishLeg optimizer parameters:\n",
    "eta_adam = 1e-4\n",
    "lr = 0.005\n",
    "beta = 0.9\n",
    "weight_decay = 1e-5\n",
    "aux_lr = 1e-4\n",
    "aux_eps = 1e-8\n",
    "update_aux_every = 10\n",
    "initialization = \"normal\"\n",
    "normalization = True\n",
    "likelihood = FISH_LIKELIHOODS[\"bernoulli\"](device=device)\n",
    "writer = SummaryWriter(\n",
    "    log_dir=f\"runs/MNIST_fishleg/lr={lr}_auxlr={aux_lr}/{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    ")\n",
    "\n",
    "# Initialising FishLeg:\n",
    "opt = FishLeg(\n",
    "    model,\n",
    "    aux_loader,\n",
    "    likelihood,\n",
    "    lr=lr,\n",
    "    beta=beta,\n",
    "    weight_decay=weight_decay,\n",
    "    aux_lr=aux_lr,\n",
    "    aux_betas=(0.9, 0.999),\n",
    "    aux_eps=aux_eps,\n",
    "    damping=damping,\n",
    "    update_aux_every=update_aux_every,\n",
    "    writer=writer,\n",
    "    method=\"antithetic\",\n",
    "    method_kwargs={\"eps\": 1e-4},\n",
    "    precondition_aux=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with FishLeg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "st = time.time()\n",
    "eval_time = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        running_loss = 0\n",
    "        for n, (batch_data, batch_labels) in enumerate(tepoch, start=1):\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            output = model(batch_data)\n",
    "\n",
    "            loss = likelihood(output, batch_labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            et = time.time()\n",
    "            if n % 50 == 0:\n",
    "                model.eval()\n",
    "\n",
    "                running_test_loss = 0\n",
    "\n",
    "                for m, (test_batch_data, test_batch_labels) in enumerate(test_loader):\n",
    "                    test_batch_data, test_batch_labels = test_batch_data.to(\n",
    "                        device\n",
    "                    ), test_batch_labels.to(device)\n",
    "\n",
    "                    test_output = model(test_batch_data)\n",
    "\n",
    "                    test_loss = likelihood(test_output, test_batch_labels)\n",
    "\n",
    "                    running_test_loss += test_loss.item()\n",
    "\n",
    "                running_test_loss /= m\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item(), test_loss=running_test_loss)\n",
    "                model.train()\n",
    "                eval_time += time.time() - et\n",
    "\n",
    "        epoch_time = time.time() - st - eval_time\n",
    "\n",
    "        tepoch.set_postfix(\n",
    "            loss=running_loss / n, test_loss=running_test_loss, epoch_time=epoch_time\n",
    "        )\n",
    "        # Write out the losses per epoch\n",
    "        writer.add_scalar(\"Loss/train\", running_loss / n, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", running_test_loss, epoch)\n",
    "\n",
    "        # Write out the losses per wall clock time\n",
    "        writer.add_scalar(\"Loss/train/time\", running_loss / n, epoch_time)\n",
    "        writer.add_scalar(\"Loss/test/time\", running_test_loss, epoch_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising ADAM optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "# betas = (0.7, 0.9)\n",
    "weight_decay = 1e-5\n",
    "# eps = 1e-8\n",
    "likelihood = FISH_LIKELIHOODS[\"bernoulli\"](device=device)\n",
    "\n",
    "opt = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    # betas=betas,\n",
    "    weight_decay=weight_decay,\n",
    "    # eps=eps,\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(\n",
    "    log_dir=f\"runs/MNIST_adam/lr={lr}_lambda={weight_decay}/{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with ADAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "st = time.time()\n",
    "eval_time = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        running_loss = 0\n",
    "        for n, (batch_data, batch_labels) in enumerate(tepoch, start=1):\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            output = model(batch_data)\n",
    "\n",
    "            loss = likelihood(output, batch_labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            et = time.time()\n",
    "            if n % 50 == 0:\n",
    "                model.eval()\n",
    "\n",
    "                running_test_loss = 0\n",
    "\n",
    "                for m, (test_batch_data, test_batch_labels) in enumerate(test_loader):\n",
    "                    test_batch_data, test_batch_labels = test_batch_data.to(\n",
    "                        device\n",
    "                    ), test_batch_labels.to(device)\n",
    "\n",
    "                    test_output = model(test_batch_data)\n",
    "\n",
    "                    test_loss = likelihood(test_output, test_batch_labels)\n",
    "\n",
    "                    running_test_loss += test_loss.item()\n",
    "\n",
    "                running_test_loss /= m\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item(), test_loss=running_test_loss)\n",
    "                model.train()\n",
    "                eval_time += time.time() - et\n",
    "\n",
    "        epoch_time = time.time() - st - eval_time\n",
    "\n",
    "        tepoch.set_postfix(\n",
    "            loss=running_loss / n, test_loss=running_test_loss, epoch_time=epoch_time\n",
    "        )\n",
    "        # Write out the losses per epoch\n",
    "        writer.add_scalar(\"Loss/train\", running_loss / n, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", running_test_loss, epoch)\n",
    "\n",
    "        # Write out the losses per wall clock time\n",
    "        writer.add_scalar(\"Loss/train/time\", running_loss / n, epoch_time)\n",
    "        writer.add_scalar(\"Loss/test/time\", running_test_loss, epoch_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FishLeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
